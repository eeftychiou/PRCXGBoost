# Project Overview

This project aims to build a Machine Learning model to predict fuel consumption for individual flight segments. The pipeline is designed to be modular, allowing for independent execution of different data processing and modeling stages.

## Data Structure and Key Files

The project's data is organized into several key directories and files, each serving a specific purpose. Understanding this structure is crucial for extending the project.

-   **`data/prc-2025-datasets/`**: This is the primary source of raw data, divided into `train`, `rank`, and `final` sets.
    -   **`flightlist_[stage].parquet`**: Contains the core flight details for each stage (`train`, `rank`, `final`). This includes `flight_id`, `aircraft_type`, `origin_icao`, `destination_icao`, and initial (uncorrected) `takeoff_timestamp` and `landing_timestamp`.
    -   **`fuel_[stage].parquet`**: Defines the flight **segments** for which fuel consumption is measured or needs to be predicted. Each flight (`flight_id`) can have multiple segments, each defined by a `start` and `end` timestamp. The `fuel_kg` is the target variable and is only present in `fuel_train.parquet`.
    -   **`trajectories/flights_[stage]/`**: Contains the raw, high-frequency trajectory data for each flight, stored in individual `[flight_id].parquet` files. These files contain time-series data like `latitude`, `longitude`, `altitude`, `groundspeed`, etc.

-   **`data/acPerf/`**: Contains aircraft performance specifications in `acPerfOpenAP.csv`, which is joined during data preparation.

-   **`data/METARs/`**: Contains raw weather data in CSV format. These files have a commented header (lines starting with `#`) and provide hourly or half-hourly weather reports for various airport stations.

-   **`data/processed/`**: This is the output directory for all processed data.
    -   **`corrected_flightlist_[stage].parquet`**: An intermediate output of the `correct_timestamps` stage. This file is crucial as it provides the corrected takeoff and landing times needed by the `prepare_metars` stage.
    -   **`processed_metars.parquet`**: The output of the `prepare_metars` stage. It contains cleaned, encoded, and imputed weather data for both departure and arrival for every flight, keyed by `flight_id`. This is a single, comprehensive file used for all subsequent runs (both test and full).
    -   **`featured_data_[stage].parquet`**: The main output of the `prepare_data` stage. These files contain the fully merged and feature-engineered dataset, ready for model training.
    -   **`feature_sets/selected_features_[model]_[method]_[timestamp].json`**: Output of the `select_features` stage, containing a JSON list of feature names chosen by the feature selection process.

-   **`data/filtered_trajectories/`**: Stores the output of the `filter_trajs` stage. These are cleaned versions of the raw trajectory files.
-   **`data/interpolated_trajectories/`**: Stores the output of the `interpolate_trajectories` stage, where missing values in the *filtered* trajectory data have been filled in.

-   **`logs/`**: Contains all log files generated by the different pipeline stages.

-   **`models/[model_name]/`**: Each trained model has its own directory containing:
    -   `model.joblib`: The trained model object.
    -   `preprocessors.joblib`: The imputer and scaler used.
    -   `selected_features.json`: The list of features the model was trained on.
    -   `evaluation_details.csv`: (Optional) A detailed breakdown of prediction errors for each segment in the validation set.

## Pipeline Stages (New Linear Workflow)

The ML pipeline is managed by `run_pipeline.py` and consists of the following stages in a new, more efficient linear order:

1.  **`profile_data`**: Analyzes the input data to generate a profile report.
2.  **`filter_trajs`**: Filters the raw trajectory files to clean up erroneous data points and saves the result to `data/filtered_trajectories/`. This is a critical first step for trajectory processing.
3.  **`interpolate_trajectories`**: Processes the *filtered* trajectory files, injects segment boundary timestamps, interpolates missing values, and saves the processed files to `data/interpolated_trajectories`.
4.  **`correct_timestamps`**: This stage loads the raw `flightlist` files, corrects the takeoff and landing times using the *interpolated* trajectory data, and saves the output as `corrected_flightlist_[stage].parquet`.
5.  **`prepare_metars`**: This stage now runs on the *full* set of `corrected_flightlist` files. It generates a single, comprehensive `processed_metars.parquet` file containing weather data for every flight, keyed by `flight_id`. This stage only needs to be run once.
6.  **`prepare_data` (Single Run)**: This stage now starts from the `corrected_flightlist` files and merges them with fuel data, aircraft performance, airport data, and the `processed_metars.parquet` file. It performs all feature engineering and saves the final `featured_data_[stage].parquet` files. It only needs to be run once.
7.  **`select_features`**: This stage loads the `featured_data_train.parquet` and performs feature selection. It supports slow but thorough SFS (`sfs_forward`, `sfs_backward`) and a much faster `'importance'` method that uses a model's built-in feature importance scores. It saves the list of selected features to a JSON file.
8.  **`train`**: Trains a machine learning model (`gbr`, `xgb`, `rf`) using a specified set of features (either all numeric or from a `selected_features.json` file).
9.  **`evaluate`**: This versatile stage has three modes, controlled by the `--run_type` argument:
    -   `--run_type evaluate`: Evaluates a trained model's performance. It can use a pre-saved validation set (from a `TEST_RUN`) or create one on the fly from the full training data. It saves a detailed `evaluation_details.csv` file for error analysis.
    -   `--run_type rank`: Generates a submission file for the `rank` dataset.
    -   `--run_type final`: Generates a submission file for the `final` dataset.
10. **`tune`**: Performs hyperparameter tuning for a selected model.

## How to Run (New Simplified Workflow)

The recommended workflow to prepare all data, select features, train, and evaluate is now a clean, linear process.

**Step 1: Filter Trajectories**
Clean the raw trajectory data.
```bash
python run_pipeline.py filter_trajs
```

**Step 2: Interpolate Trajectories**
Interpolate the cleaned trajectory data.
```bash
python run_pipeline.py interpolate_trajectories
```

**Step 3: Correct Timestamps**
Generate the essential `corrected_flightlist` files using the interpolated data.
```bash
python run_pipeline.py correct_timestamps
```

**Step 4: Prepare METAR Data**
Create the single, complete `processed_metars.parquet` file.
```bash
python run_pipeline.py prepare_metars
```

**Step 5: Final Data Preparation**
Merge all data sources to produce the final `featured_data` files.
```bash
python run_pipeline.py prepare_data
```

**Step 6: Select Features (Optional but Recommended)**
Run `select_features` to get an optimized feature set.
```bash
# Fast method
python run_pipeline.py select_features --fs_model xgb --fs_method importance
# Example output: processed/feature_sets/selected_features_xgb_importance_20251122-123456.json
```

**Step 7: Train the Model**
Train your model, providing the path to the selected features file.
```bash
python run_pipeline.py train --model xgb --features "processed/feature_sets/selected_features_xgb_importance_20251122-123456.json"
```

**Step 8: Evaluate and Generate Submissions**
Evaluate performance and/or generate submission files.
```bash
# Evaluate performance and get detailed error CSV
python run_pipeline.py evaluate --run_type evaluate

# Generate submission for the rank dataset
python run_pipeline.py evaluate --run_type rank

# Generate submission for the final dataset
python run_pipeline.py evaluate --run_type final
```

## Configuration

-   **`TEST_RUN`**: Set to `True` in `config.py` to run the `prepare_data`, `select_features`, and `train` stages on a small fraction of the data for faster testing and debugging. Note that the `prepare_metars` stage now always runs on the full dataset to produce a complete weather file.
-   **`TEST_RUN_FRACTION`**: The fraction of data to use when `TEST_RUN` is `True`.
-   **`ENABLE_PHASE_DURATION_FEATURES`**: Set to `True` in `config.py` to use the experimental time-based phase duration and fraction features. Set to `False` to use the original point-count-based phase fractions.
-   **`USE_OPENAP_PHASE_DETECTION`**: Set to `True` in `config.py` to use the `openap` library for phase detection during the `interpolate_trajectories` stage.
