# Project Overview

This project aims to build a Machine Learning model to predict fuel consumption for individual flight segments. The pipeline is designed to be modular, allowing for independent execution of different data processing and modeling stages.

## Data Structure and Key Files

The project's data is organized into several key directories and files, each serving a specific purpose. Understanding this structure is crucial for extending the project.

-   **`data/prc-2025-datasets/`**: This is the primary source of raw data, divided into `train`, `rank`, and `final` sets.
    -   **`flightlist_[stage].parquet`**: Contains the core flight details for each stage (`train`, `rank`, `final`). This includes `flight_id`, `aircraft_type`, `origin_icao`, `destination_icao`, and initial (uncorrected) `takeoff_timestamp` and `landing_timestamp`.
    -   **`fuel_[stage].parquet`**: Defines the flight **segments** for which fuel consumption is measured or needs to be predicted. Each flight (`flight_id`) can have multiple segments, each defined by a `start` and `end` timestamp. The `fuel_kg` is the target variable and is only present in `fuel_train.parquet`.
    -   **`trajectories/flights_[stage]/`**: Contains the raw, high-frequency trajectory data for each flight, stored in individual `[flight_id].parquet` files. These files contain time-series data like `latitude`, `longitude`, `altitude`, `groundspeed`, etc.
    -   **`apt.parquet`**: Contains airport data, including ICAO codes, coordinates, and elevation. This file is enriched by the `impute_apt.py` script.

-   **`data/acPerf/`**: Contains aircraft performance specifications in `acPerfOpenAP.csv`, which is joined during data preparation.

-   **`data/METARs/`**: Contains raw weather data in CSV format. These files have a commented header (lines starting with `#`) and provide hourly or half-hourly weather reports for various airport stations.

-   **`data/processed/`**: This is the output directory for all processed data.
    -   **`corrected_flightlist_[stage].parquet`**: An intermediate output of the `correct_timestamps` stage. This file is crucial as it provides the corrected takeoff and landing times needed by the `prepare_metars` stage.
    -   **`featured_data_[stage].parquet`**: The main output of the `prepare_data` stage. These files contain the fully merged and feature-engineered dataset, including corrected timestamps.
    -   **`processed_metars.parquet`**: The output of the `prepare_metars` stage. It contains cleaned, encoded, and imputed weather data aligned with the flights from the `featured_data` files.
    -   **`average_load_factor_by_airport_pair_v3.csv`**: The output of the `regionalLoadFactor.py` script. Contains estimated passenger load factors for each unique airport pair.

-   **`data/interpolated_trajectories/`**: Stores the output of the `interpolate_trajectories` stage, where missing values in the raw trajectory data have been filled in.

-   **`logs/`**: Contains all log files generated by the different pipeline stages.

## Pre-processing Stages

These scripts should be run once before the main pipeline to prepare and enrich the core datasets.

1.  **`impute_apt.py`**: This script enriches the `apt.parquet` file by scraping SkyVector for detailed airport information, such as runway lengths, headings, and elevations. It is designed to be run once to fill in missing data. It is idempotent, meaning it can be run multiple times without causing issues, as it restores from a backup of the original file on each run.

2.  **`regionalLoadFactor.py`**: This script estimates the average passenger load factor for every unique origin-destination airport pair found in the flight data. It uses IATA regional statistics and saves the output to `data/processed/average_load_factor_by_airport_pair_v3.csv`. This file can then be merged during the `prepare_data` stage to be used as a feature.

## Pipeline Stages

The ML pipeline is managed by `run_pipeline.py` and consists of the following stages:

1.  **`profile_data`**: Analyzes the input data to generate a profile report.
2.  **`filter_trajectories`**: Filters raw trajectory files to remove erroneous data points.
3.  **`interpolate_trajectories`**: Processes the *filtered* trajectory files, interpolates missing values, and saves the processed files to `data/interpolated_trajectories`.
4.  **`correct_timestamps`**: This stage loads the raw `flightlist` files, corrects the takeoff and landing times using the interpolated trajectory data, and saves the output as `corrected_flightlist_[stage].parquet`.
5.  **`prepare_metars`**: This stage reads the `corrected_flightlist` files from the `processed` directory to get the corrected flight details. It then loads the raw METAR data, finds the nearest weather station for each airport (within a 100nm radius), and generates a clean `processed_metars.parquet` file with encoded and imputed weather features.
6.  **`prepare_data`**: This stage merges the `corrected_flightlist`, fuel data, airport data, aircraft performance, and the `processed_metars.parquet` file. It performs all feature engineering and saves the final `featured_data_[stage].parquet` files.
7.  **`train`**: Trains a machine learning model (`gbr`, `xgb`, `rf`).
8.  **`predict`**: Uses a trained model to make predictions and create a submission file.
9.  **`evaluate`**: Evaluates the performance of a trained model.
10. **`tune`**: Performs hyperparameter tuning for a selected model.

## How to Run (Updated Workflow)

The recommended workflow is as follows:

**Step 1: Run Pre-processing Scripts (Run Once)**
First, enrich the core datasets.

```bash
# Scrape airport data to enrich apt.parquet
python impute_apt.py

# Generate the regional load factor estimates
python regionalLoadFactor.py
```

**Step 2: Correct Timestamps**
Generate the essential `corrected_flightlist` files.

```bash
python run_pipeline.py correct_timestamps
```

**Step 3: Prepare METAR Data**
Create the `processed_metars.parquet` file using the corrected timestamps.

```bash
python run_pipeline.py prepare_metars
```

**Step 4: Final Data Preparation**
Merge all data sources, including the newly created weather and load factor data, to produce the final `featured_data` files.

```bash
python run_pipeline.py prepare_data
```

After these steps, you can proceed with training the model:

```bash
python run_pipeline.py train --model xgb
```

## Configuration

-   **`TEST_RUN`**: Set to `True` in `config.py` to run the pipeline on a small fraction of the data for faster testing and debugging.
-   **`TEST_RUN_FRACTION`**: The fraction of data to use when `TEST_RUN` is `True`.
